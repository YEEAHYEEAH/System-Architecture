# 웹 크롤계

## 개략적 설계

### 💬 웹 크롤러의 이용 식

웹 크롤러는 로봇 또는 스파이더라고도 불립니다. 검색 엔진에서 널리 쓰이는 기술로, 웹에 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적입니다. 여기서 콘텐츠는 웹 페이지일 수도 있고, 이미지나 비디오 같은 정적 파일일 수도 있습니다. 크롤러는 아래와 같이 다양한 방면으로 이용될 수 있습니다.

- 검색 엔진 인덱싱: 크롤러가 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 생성하는 것으로, 대표적인 예시인 Googlebot이 있다.
- 웹 아카이빙: 나중에 사용할 목적으로 웹을 장기보관하기 위해 웹에서 정보를 모으는 절차이다.
- 웹 마이닝: 데이터 마이닝 업계에 활용될 데이터를 모으기 위한 크롤링이다.
- 웹 모니터링: 크롤러를 사용해 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링할 수 있다.

### 💬 개략적 설계안 제시

![](https://seongwon.dev/static/a1437214d7a2db9f137241ec10311cf7/97a96/img.png)

❕**시작 URL 집합**

웹 크롤러가 크롤링을 시작하는 촐발점입니다.

❕**미수집 URL 저장소**

대부분의 현대적인 웹 크롤러는 크롤링 상태를 다운로드 할 URL과 다운로드된 URL 두 가지의 종류로 나눠 관리합니다. 이 중 다운로드 할 URL을 관리하는 컴포넌트를 미수집URL이라고 합니다. 편히 말해 Queue라고 생각하면 됩니다. 

**❕HTML 다운로더**

HTML 다운로더는 인터넷에서 웹 페이지를 다운로드하는 컴포넌트입니다. 다운로드할 페이지의 URL은 미숮비 URL 저장소가 제공합니다.

❕**도메인 이름 변환기**

웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요합니다. HTML 다운로더는 도메인 이름 변환기를 사용해서 URL 주소에 대응되는 IP 주소를 알아냅니다.

❕**콘텐츠 파서**

웹 페이지를 다운로드하면 파싱과 검증 절차를 거쳐야 합니다. 이상한 웹 페이지를 다운로드했다가 문제가 발생할 수도 있고, 저장 공간을 낭비할 수도 있기 때문입니다. 크롤링 서버 안에 파서를 구현하면 크롤링 과정이 너무 느려지기 때문에, 독립된 컴포넌트로 구현합니다. 

❕**중복 콘텐츠인가?**

중복된 웹 페이지에 대한 크롤링 작업을 줄이기 위해 자료 구졸르 도입하여 데이터 처리 소요 시간을 감소시킨다. 두 HTML 문서를 비교하는 게 가장 간단하겠지만 문서가 많고 클 경우 느리고 비효율적이다. 효과적인 방법은 웹 페이지의 해시값을 비교하는 것이다. 

❕**콘텐츠 저장소**

HTML 문서를 보관하는 시스템이다. 데이터의 양이 너무 많기 떄문에 대부분의 콘텐츠는 디스크에 저장한다. 하지만 인기 있는 콘텐츠는 메모리에 두어 접근 지연 시간을 줄인다. 

❕**URL 추출기**

HTML을 파싱하여 링크들을 골라내는 역할을 한다. 즉, < a > 태그에 있는 링크들을 절대 경로로 바꾸어 저장하는 것이다. 

❕**URL 필터**

URL 필터는 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 합니다. 

❕**이미 방문한 URL?**

이 단계를 구현하기 위해서는 이미 방문한 URL이나 미수집URL 저장소에 보관된 URL을 추적할 수 있도록 해야 합니다. 이를 위해서 블룸 필터나 해시 테이블이 널리 사용됩니다.

❕**URL 저장소**

이미 방문한 URL을 보관하는 저장소입니다.세 설계

## 상세 설계

### 💬 DFS vs BFS

웹은 마치 유향 그래프와 같습니다. 페이지는 노드이고 하이퍼링크는 에지라고 보면 됩니다. 크롤링 프로세스는 이 유향 그래프를 따라 탐색하는 과정입니다. 여기서 선택지가 두 개 존재하는데요, 바로 DFS와 BFS입니다. 

하지만 DFS는 좋은 선택이 아닐 가능성이 높습니다. 그래프의 크기가 클 경우 어느 정도로 깊게 들어가야 할지 가늠이 어렵다는 문제점이 있기 때문입니다. 따라서 웹 크롤러는 BFS를 사용하는데, 이 구현법에서는 FIFO를 사용하기 때문에 발생하는 문제점이 있습니다.

1. 한 페이지에서 나오는 링크의 상당수는 같은 페이지로 되돌아간다. 
    
    ![](https://velog.velcdn.com/images/dev_dc_hyeon/post/53f6cecd-1560-4d1f-866b-7e3bb7df7732/image.png)
    
2. 표준적 BFS 알고리즘은 URL 간의 우선 순위를 두지 않는다.

### 💬 미수집 URL 저장소

미수집 URL 저장소를 활용하면 이러한 문제점을 조금 더 쉽게 해결할 수 있습니다. 이 저장소를 잘 구현하면 무차별적인 요청을 보내지 않는 예의 있는 크롤러를 만들 수 있습니다. 여기서 말하는 예의란, 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다는 것입니다. 같은 웹 사이트의 페이지를 다운받는 태스크는 시간차를 두고 실행하도록 하는 것입니다. 즉 각 다운로드 스레드마다 FIFO 큐를 가지고 있어서, 해당 큐에서 꺼낸 URL만 다운로드하도록 하는 것입니다.

![](https://velog.velcdn.com/images/dev_dc_hyeon/post/fb5da30d-a2a4-4d77-bf84-86ed8c5c3989/image.png)

- 순위결정장치: URL을 입력으로 받아 페이지 랭크, 트래픽 양, 갱신 빈도 등을 척도로 우선순위를 계산합니다.
- 전면 큐 선택기: 처리할 URL을 꺼내는 역할을 하며, 높은 우선순위의 큐에서 더 자주 추출됩니다.
- 후면 큐 라우터: 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장하는 역할을 합니다.
- 후면 큐 선택기: 큐들을 순회하면서 큐에서 URL을 꺼내 다운로드하도록 작업 스레드에 저장합니다.
- 작업 스레드: 전달받은 URL을 다운로드하는 작업을 수행합니다.

### 💬 HTML 다운로더

HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려받습니다. 다운로더에 대해 알아보기 전에 로봇 제외 프로토콜에 대해 먼저 이야기해 보겠습니다. 

❕**Robots.txt**

로봇 제외 프로토콜이라고 부르기도 하는 Robots.txt는 웹사이트가 크롤러와 소통하는 표준적인 방법입니다. 이 파일에는 크롤러가 수집해도 되는 페이지 목록들이 들어 있습니다. 따라서 웹 사이트를 다운로드 하기 전에 해당 파일에 나열된 규칙들을 확인해야 합니다. 

이제 다시 HTML 다운로드에 대해 이야기해 볼까요? HTML 다운로더를 설계할 때는 성능 최적화가 필수적입니다. 아래와 같은 성능 최적화 기법을 사용해 볼 수 있습니다. 

❕**성능 최적화**

1. 분산 크롤링: 성능을 높이기 위해 크롤링 작업을 여러 서버에 분산하는 방법입니다.
2. 도메인 이름 변환 결과 캐시: DNS Resolver는 크롤러의 병목 원인 중 하나인데, 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 두고 cron job을 통해 주기적으로 갱신하는 방법입니다.
3. 지역성: 크롤링 작업을 수행하는 서버를 지역마다 분산하는 방법입니다.
4. 짧은 타임아웃: 최대 얼마나 기다릴지를 미리 정해 두고 대기 시간을 줄이는 방법입니다.

### 💬 문제 있는 콘텐츠 감지 및 회략

중복된 웹 페이지거나 의미 없는 또는 유해한 콘텐츠를 어떻게 감지하고 시스템으로부터 차단할 수 있을까요? 

**❕중복 콘텐츠**

해시나 체크섬을 사용하여 중복 콘텐츠의 다운로드를 회피합니다.  

**❕거미 덫**

거미덫은 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지입니다. 예를 들어

`sample.com/bar/foo/bar/foo/bar/foo/bar/foo/bar/foo/bar/foo/bar/foo/…`

를 들 수 있습니다. 이런 덫은 URL의 최대 길이를 제한하면 회피할 수 있습니다. 하지만 요즈음의 URL은 정말 긴 경우도 많아서, 사람이 수작업으로 덫을 확인하고 찾아낸 후 거미덫 사이트를 크롤러 탐색에서 제외하는 방법을 사용합니다. 

**❕데이터 노이즈**

광고나 스크립트 코드, 스팸 URL은 크롤러에게 도움되지 않기 떄문에 가능하다면 제외합니다.
